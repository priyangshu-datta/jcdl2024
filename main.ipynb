{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/datta/Documents/Github/jcdl2024/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in module\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from src.rag_dme import extract_datasets, GenerativeServiceClient, ChromaPersist\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaDB = ChromaPersist(name=\"embeds\", path=Path(\".local\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722333071.114164     889 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    }
   ],
   "source": [
    "gsc = GenerativeServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmdd = pl.read_json(\".local/queries.json\")[350:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:47<00:00, 47.10s/it]\n"
     ]
    }
   ],
   "source": [
    "for paper_id, content in tqdm(dmdd[[\"paper_id\", \"content\"]][1:2].transpose().to_dict().values()):\n",
    "    try:\n",
    "        start = perf_counter()\n",
    "        datasets = extract_datasets(\n",
    "                    chromaDB=chromaDB, \n",
    "                    gsc=gsc, \n",
    "                    full_text=content\n",
    "                )\n",
    "        end = perf_counter()\n",
    "        result.append(\n",
    "            {\n",
    "                \"paper_id\": paper_id, \n",
    "                \"observed\": datasets,\n",
    "                \"time_elapsed\": end-start\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_id': '209202273',\n",
       "  'observed': ['LSUN CAR',\n",
       "   'LSUN HORSE',\n",
       "   'LSUN CAT',\n",
       "   'FFHQ',\n",
       "   'LSUN CHURCH',\n",
       "   'LSUN'],\n",
       "  'time_elapsed': 49.18447363200002}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FFHQ, Cars, LPIPS, CAT, CHURCH, StyleGAN, CAR, LSUN CAR, LSUN, HORSE'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_ndjson(\".local/350-450.jsonl\")[1][\"observed\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The style-based GAN architecture (StyleGAN) yields state-of-the-art results '\n",
      " 'in data-driven unconditional generative image modeling. We expose and '\n",
      " 'analyze several of its characteristic artifacts, and propose changes in both '\n",
      " 'model architecture and training methods to address them. In particular, we '\n",
      " 'redesign the generator normalization, revisit progressive growing, and '\n",
      " 'regularize the generator to encourage good conditioning in the mapping from '\n",
      " 'latent codes to images. In addition to improving image quality, this path '\n",
      " 'length regularizer yields the additional benefit that the generator becomes '\n",
      " 'significantly easier to invert. This makes it possible to reliably attribute '\n",
      " 'a generated image to a particular network. We furthermore visualize how well '\n",
      " 'the generator utilizes its output resolution, and identify a capacity '\n",
      " 'problem, motivating us to train larger models for additional quality '\n",
      " 'improvements. Overall, our improved model redefines the state of the art in '\n",
      " 'unconditional image modeling, both in terms of existing distribution quality '\n",
      " 'metrics as well as perceived image quality. The resolution and quality of '\n",
      " 'images produced by generative methods, especially generative adversarial '\n",
      " 'networks (GAN) [15] , are improving rapidly [23, 31, 5] . The current '\n",
      " 'state-of-the-art method for high-resolution image synthesis is StyleGAN [24] '\n",
      " ', which has been shown to work reliably on a variety of datasets. Our work '\n",
      " 'focuses on fixing its characteristic artifacts and improving the result '\n",
      " 'quality further. The distinguishing feature of StyleGAN [24] is its '\n",
      " 'unconventional generator architecture. Instead of feeding the input latent '\n",
      " 'code z ∈ Z only to the beginning of a the network, the mapping network f '\n",
      " 'first transforms it to an intermediate latent code w ∈ W. Affine transforms '\n",
      " 'then produce styles that control the layers of the synthesis network g via '\n",
      " 'adaptive instance normalization (AdaIN) [20, 9, 12, 8] . Additionally, '\n",
      " 'stochastic variation is facilitated by providing additional random noise '\n",
      " 'maps to the synthesis network. It has been demonstrated [24, 38] that this '\n",
      " 'design allows the intermediate latent space W to be much less entangled than '\n",
      " 'the input latent space Z. In this paper, we focus all analysis solely on W, '\n",
      " \"as it is the relevant latent space from the synthesis network's point of \"\n",
      " 'view. Many observers have noticed characteristic artifacts in images '\n",
      " 'generated by StyleGAN [3] . We identify two causes for these artifacts, and '\n",
      " 'describe changes in architecture and training methods that eliminate them. '\n",
      " 'First, we investigate the origin of common blob-like artifacts, and find '\n",
      " 'that the generator creates them to circumvent a design flaw in its '\n",
      " 'architecture. In Section 2, we redesign the normalization used in the '\n",
      " 'generator, which removes the artifacts. Second, we analyze artifacts related '\n",
      " 'to progressive growing [23] that has been highly successful in stabilizing '\n",
      " 'high-resolution GAN training. We propose an alternative design that achieves '\n",
      " 'the same goal -training starts by focusing on low-resolution images and then '\n",
      " 'progressively shifts focus to higher and higher resolutions -without '\n",
      " 'changing the network topology during training. This new design also allows '\n",
      " 'us to reason about the effective resolution of the generated images, which '\n",
      " 'turns out to be lower than expected, motivating a capacity increase (Section '\n",
      " '4). Quantitative analysis of the quality of images produced using generative '\n",
      " 'methods continues to be a challenging topic. Fréchet inception distance '\n",
      " '(FID) [19] measures differences in the density of two distributions in the '\n",
      " 'highdimensional feature space of a InceptionV3 classifier [39] . Precision '\n",
      " 'and Recall (P&R) [36, 27] provide additional visibility by explicitly '\n",
      " 'quantifying the percentage of generated images that are similar to training '\n",
      " 'data and the percentage of training data that can be generated, '\n",
      " 'respectively. We use these metrics to quantify the improvements. Both FID '\n",
      " 'and P&R are based on classifier networks that have recently been shown to '\n",
      " 'focus on textures rather than shapes [11] , and consequently, the metrics do '\n",
      " 'not accurately capture all aspects of image quality. We observe that the '\n",
      " 'perceptual path length (PPL) metric [24] , introduced as a method for '\n",
      " 'estimating the quality of latent space interpo- Figure 1 . Instance '\n",
      " 'normalization causes water droplet -like artifacts in StyleGAN images. These '\n",
      " 'are not always obvious in the generated images, but if we look at the '\n",
      " 'activations inside the generator network, the problem is always there, in '\n",
      " 'all feature maps starting from the 64x64 resolution. It is a systemic '\n",
      " 'problem that plagues all StyleGAN images. lations, correlates with '\n",
      " 'consistency and stability of shapes. Based on this, we regularize the '\n",
      " 'synthesis network to favor smooth mappings (Section 3) and achieve a clear '\n",
      " 'improvement in quality. To counter its computational expense, we also '\n",
      " 'propose executing all regularizations less frequently, observing that this '\n",
      " 'can be done without compromising effectiveness. Finally, we find that '\n",
      " 'projection of images to the latent space W works significantly better with '\n",
      " 'the new, pathlength regularized generator than with the original Style-GAN. '\n",
      " 'This has practical importance since it allows us to tell reliably whether a '\n",
      " 'given image was generated using a particular generator (Section 5). Our '\n",
      " 'implementation and trained models are available at '\n",
      " 'https://github.com/NVlabs/stylegan2 We begin by observing that most images '\n",
      " 'generated by StyleGAN exhibit characteristic blob-shaped artifacts that '\n",
      " 'resemble water droplets. As shown in Figure 1 , even when the droplet may '\n",
      " 'not be obvious in the final image, it is present in the intermediate feature '\n",
      " 'maps of the generator. 1 The anomaly starts to appear around 64×64 '\n",
      " 'resolution, is present in all feature maps, and becomes progressively '\n",
      " 'stronger at higher resolutions. The existence of such a consistent artifact '\n",
      " 'is puzzling, as the discriminator should be able to detect it. We pinpoint '\n",
      " 'the problem to the AdaIN operation that normalizes the mean and variance of '\n",
      " 'each feature map separately, thereby potentially destroying any information '\n",
      " 'found in the magnitudes of the features relative to each other. We '\n",
      " 'hypothesize that the droplet artifact is a result of the generator '\n",
      " 'intentionally sneaking signal strength information past instance '\n",
      " 'normalization: by creating a strong, localized spike that dominates the '\n",
      " 'statistics, the generator can effectively scale the signal as it likes '\n",
      " 'elsewhere. Our hypothesis is supported by the finding that when the '\n",
      " 'normalization step is removed from the generator, as detailed below, the '\n",
      " 'droplet artifacts disappear completely. We will first revise several details '\n",
      " 'of the StyleGAN generator to better facilitate our redesigned normalization. '\n",
      " 'These changes have either a neutral or small positive effect on their own in '\n",
      " 'terms of quality metrics. Figure 2a shows the original StyleGAN synthesis '\n",
      " 'network g [24] , and in Figure 2b we expand the diagram to full detail by '\n",
      " 'showing the weights and biases and breaking the AdaIN operation to its two '\n",
      " 'constituent parts: normalization and modulation. This allows us to re-draw '\n",
      " 'the conceptual gray boxes so that each box indicates the part of the network '\n",
      " 'where one style is active (i.e., \"style block\"). Interestingly, the original '\n",
      " 'StyleGAN applies bias and noise within the style block, causing their '\n",
      " \"relative impact to be inversely proportional to the current style's \"\n",
      " 'magnitudes. We observe that more predictable results are obtained by moving '\n",
      " 'these operations outside the style block, where they operate on normalized '\n",
      " 'data. Furthermore, we notice that after this change it is sufficient for the '\n",
      " 'normalization and modulation to operate on the standard deviation alone '\n",
      " '(i.e., the mean is not needed). The application of bias, noise, and '\n",
      " 'normalization to the constant input can also be safely removed without '\n",
      " 'observable drawbacks. This variant is shown in Figure 2c , and serves as a '\n",
      " 'starting point for our redesigned normalization. Given that instance '\n",
      " 'normalization appears to be too strong, how can we relax it while retaining '\n",
      " 'the scale-specific effects of the styles? We rule out batch normalization '\n",
      " '[21] as it is incompatible with the small minibatches required for '\n",
      " 'high-resolution synthesis. Alternatively, we could simply remove the '\n",
      " 'normalization. While actually improving FID slightly [27] , this makes the '\n",
      " 'effects of the styles cumulative rather than scale-specific, essentially '\n",
      " 'losing the controllability offered by StyleGAN (see video). We will now '\n",
      " 'propose an alternative that removes the artifacts while retaining '\n",
      " 'controllability. The main idea is to base normalization on the expected '\n",
      " 'statistics of the incoming feature maps, but without explicit forcing. '\n",
      " 'Recall that a style block in Figure 2c consists of modulation, convolution, '\n",
      " 'and normalization. Let us start by consid- Here we have broken the AdaIN to '\n",
      " 'explicit normalization followed by modulation, both operating on the mean '\n",
      " 'and standard deviation per feature map. We have also annotated the learned '\n",
      " 'weights (w), biases (b), and constant input (c), and redrawn the gray boxes '\n",
      " 'so that one style is active per box. The activation function (leaky ReLU) is '\n",
      " 'always applied right after adding the bias. (c) We make several changes to '\n",
      " 'the original architecture that are justified in the main text. We remove '\n",
      " 'some redundant operations at the beginning, move the addition of b and B to '\n",
      " 'be outside active area of a style, and adjust only the standard deviation '\n",
      " 'per feature map. (d) The revised architecture enables us to replace instance '\n",
      " 'normalization with a \"demodulation\" operation, which we apply to the weights '\n",
      " 'associated with each convolution layer. ering the effect of a modulation '\n",
      " 'followed by a convolution. The modulation scales each input feature map of '\n",
      " 'the convolution based on the incoming style, which can alternatively be '\n",
      " 'implemented by scaling the convolution weights: where w and w are the '\n",
      " 'original and modulated weights, respectively, s i is the scale corresponding '\n",
      " 'to the ith input feature map, and j and k enumerate the output feature maps '\n",
      " 'and spatial footprint of the convolution, respectively. Now, the purpose of '\n",
      " 'instance normalization is to essentially remove the effect of s from the '\n",
      " \"statistics of the convolution's output feature maps. We observe that this \"\n",
      " 'goal can be achieved more directly. Let us assume that the input activations '\n",
      " 'are i.i.d. random variables with unit standard deviation. After modulation '\n",
      " 'and convolution, the output activations have standard deviation of i.e., the '\n",
      " 'outputs are scaled by the L 2 norm of the corresponding weights. The '\n",
      " 'subsequent normalization aims to restore the outputs back to unit standard '\n",
      " 'deviation. Based on Equation 2, this is achieved if we scale (\"demodulate\") '\n",
      " 'each output feature map j by 1/σ j . Alternatively, we can again bake this '\n",
      " 'into the convolution weights: where is a small constant to avoid numerical '\n",
      " 'issues. We have now baked the entire style block to a single convolution '\n",
      " 'layer whose weights are adjusted based on s using Equations 1 and 3 ( Figure '\n",
      " '2d ). Compared to instance normalization, our demodulation technique is '\n",
      " 'weaker because it is based on statistical assumptions about the signal '\n",
      " 'instead of actual contents of the feature maps. Similar statistical analysis '\n",
      " 'has been extensively used in modern network initializers [13, 18] , but we '\n",
      " 'are not aware of it being previously used as a replacement for '\n",
      " 'data-dependent normalization. Our demodulation is also related to weight '\n",
      " 'normalization [37] that performs the same calculation as a part of '\n",
      " 'reparameterizing the weight tensor. Prior work has identified weight '\n",
      " 'normalization as beneficial in the context of GAN training [42] . Our new '\n",
      " 'design removes the characteristic artifacts ( (Table 1 , rows A, B), but '\n",
      " 'there is a notable shift from precision to recall. We argue that this is '\n",
      " 'generally desirable, since recall can be traded into precision via '\n",
      " 'truncation, whereas the opposite is not true [27] . In practice our design '\n",
      " 'can be Table 1 . Main results. For each training run, we selected the '\n",
      " 'training snapshot with the lowest FID. We computed each metric 10 times with '\n",
      " 'different random seeds and report their average. The \"path length\" column '\n",
      " 'corresponds to the PPL metric, computed based on path endpoints in W [24] . '\n",
      " 'For LSUN datasets, we report path lengths without the center crop that was '\n",
      " 'originally proposed for FFHQ. The FFHQ dataset contains 70k images, and we '\n",
      " 'showed the discriminator 25M images during training. For LSUN CAR the '\n",
      " 'corresponding numbers were 893k and 57M. implemented efficiently using '\n",
      " 'grouped convolutions, as detailed in Appendix B. To avoid having to account '\n",
      " 'for the activation function in Equation 3, we scale our activation functions '\n",
      " 'so that they retain the expected signal variance. While GAN metrics such as '\n",
      " 'FID or Precision and Recall (P&R) successfully capture many aspects of the '\n",
      " 'generator, they continue to have somewhat of a blind spot for image quality. '\n",
      " 'For an example, refer to We observe an interesting correlation between '\n",
      " 'perceived image quality and perceptual path length (PPL) [24] , a metric '\n",
      " 'that was originally introduced for quantifying the smoothness of the mapping '\n",
      " 'from a latent space to the output image by measuring average LPIPS distances '\n",
      " '[49] between generated images under small perturbations in latent space. '\n",
      " 'Again consulting Figures 13 and 14 , a smaller PPL (smoother generator '\n",
      " 'mapping) appears to correlate with higher overall image quality, whereas '\n",
      " 'other metrics are blind to the change. Figure 4 examines this correlato be '\n",
      " 'biased towards texture detection. As such, images with, e.g., strong cat '\n",
      " 'textures may appear more similar to each other than a human observer would '\n",
      " 'agree, thus partially compromising density-based metrics (FID) and manifold '\n",
      " 'coverage metrics (P&R). tion more closely through per-image PPL scores '\n",
      " 'computed by sampling latent space around individual points in W on StyleGAN '\n",
      " 'trained on LSUN CAT: low PPL scores are indeed indicative of high-quality '\n",
      " 'images, and vice versa. Figure 5a shows the corresponding histogram of '\n",
      " 'per-image PPL scores and reveals the long tail of the distribution. The '\n",
      " 'overall PPL for the model is simply the expected value of the per-image PPL '\n",
      " 'scores. It is not immediately obvious why a low PPL should correlate with '\n",
      " 'image quality. We hypothesize that during training, as the discriminator '\n",
      " 'penalizes broken images, the most direct way for the generator to improve is '\n",
      " 'to effectively stretch the region of latent space that yields good images. '\n",
      " 'This would lead to the low-quality images being squeezed into small latent '\n",
      " 'space regions of rapid change. While this improves the average output '\n",
      " 'quality in the short term, the accumulating distortions impair the training '\n",
      " 'dynamics and consequently the final image quality. This empirical '\n",
      " 'correlation suggests that favoring a smooth generator mapping by encouraging '\n",
      " 'low PPL during training may improve image quality, which we show to be the '\n",
      " 'case below. As the resulting regularization term is somewhat expensive to '\n",
      " 'compute, we first describe a general optimization that applies to all '\n",
      " 'regularization techniques. Typically the main loss function (e.g., logistic '\n",
      " 'loss [15] ) and regularization terms (e.g., R 1 [30] ) are written as a '\n",
      " 'single expression and are thus optimized simultaneously. We observe that '\n",
      " 'typically the regularization terms can be computed much less frequently than '\n",
      " 'the main loss function, thus greatly diminishing their computational cost '\n",
      " 'and the overall memory usage. Table 1 , row C shows that no harm is caused '\n",
      " 'when R 1 regularization is performed only once every 16 minibatches, and we '\n",
      " 'adopt the same strategy for our new regularizer as well. Appendix B gives '\n",
      " 'implementation details. Excess path distortion in the generator is evident '\n",
      " 'as poor local conditioning: any small region in W becomes arbitrarily '\n",
      " 'squeezed and stretched as it is mapped by g. In line with earlier work [33] '\n",
      " ', we consider a generator mapping from the latent space to image space to be '\n",
      " 'well-conditioned if, at each point in latent space, small displacements '\n",
      " 'yield changes of equal magnitude in image space regardless of the direction '\n",
      " 'of perturbation. At a single w ∈ W, the local metric scaling properties of '\n",
      " 'the generator mapping g(w) : W → Y are captured by the Jacobian matrix J w = '\n",
      " '∂g(w)/∂w. Motivated by the desire to preserve the expected lengths of '\n",
      " 'vectors regardless of the direction, we formulate our regularizer as Figure '\n",
      " '6 . Progressive growing leads to \"phase\" artifacts. In this example the '\n",
      " 'teeth do not follow the pose but stay aligned to the camera, as indicated by '\n",
      " 'the blue line. where y are random images with normally distributed pixel '\n",
      " 'intensities, and w ∼ f (z), where z are normally distributed. We show in '\n",
      " 'Appendix C that, in high dimensions, this prior is minimized when J w is '\n",
      " 'orthogonal (up to a global scale) at any w. An orthogonal matrix preserves '\n",
      " 'lengths and introduces no squeezing along any dimension. To avoid explicit '\n",
      " 'computation of the Jacobian matrix, we use the identity J T w y = ∇ w (g(w) '\n",
      " '· y), which is efficiently computable using standard backpropagation [6] . '\n",
      " 'The constant a is set dynamically during optimization as the long-running '\n",
      " 'exponential moving average of the lengths J T w y 2 , allowing the '\n",
      " 'optimization to find a suitable global scale by itself. Our regularizer is '\n",
      " 'closely related to the Jacobian clamping regularizer presented by Odena et '\n",
      " 'al. [33] . Practical differences include that we compute the products J T w '\n",
      " 'y analytically whereas they use finite differences for estimating J w δ with '\n",
      " 'Z δ ∼ N (0, I). It should be noted that spectral normalization [31] of the '\n",
      " 'generator [45] only constrains the largest singular value, posing no '\n",
      " 'constraints on the others and hence not necessarily leading to better '\n",
      " 'conditioning. In practice, we notice that path length regularization leads '\n",
      " 'to more reliable and consistently behaving models, making architecture '\n",
      " 'exploration easier. Figure 5b shows that path length regularization clearly '\n",
      " 'improves the distribution of per-image PPL scores. Table 1 , row D shows '\n",
      " 'that regularization reduces PPL, as expected, but there is a tradeoff '\n",
      " 'between FID and PPL in LSUN CAR and other datasets that are less structured '\n",
      " 'than FFHQ. Furthermore, we observe that a smoother generator is easier to '\n",
      " 'invert (Section 5). Progressive growing [23] has been very successful in '\n",
      " 'stabilizing high-resolution image synthesis, but it causes its own '\n",
      " 'characteristic artifacts. The key issue is that the progressively grown '\n",
      " 'generator appears to have a strong location preference for details; the '\n",
      " 'accompanying video shows that when features like teeth or eyes should move '\n",
      " 'smoothly over the image, they may instead remain stuck in place before '\n",
      " 'jumping to the next preferred location. Figure 6 shows a related artifact. '\n",
      " 'We believe the problem is that in progressive growing each resolution serves '\n",
      " 'momentarily as the output resolution, forcing it to generate maximal '\n",
      " 'frequency details, which then leads to the trained network to have '\n",
      " 'excessively high frequencies in the intermediate layers, compromising shift '\n",
      " 'invariance [48] . Appendix A shows an example. These issues prompt us to '\n",
      " 'search for an alternative formulation that would retain the benefits of '\n",
      " 'progressive growing without the drawbacks. While StyleGAN uses simple '\n",
      " 'feedforward designs in the generator (synthesis network) and discriminator, '\n",
      " 'there is a vast body of work dedicated to the study of better network '\n",
      " 'architectures. In particular, skip connections [34, 22] , residual networks '\n",
      " '[17, 16, 31] , and hierarchical methods [7, 46, 47] have proven highly '\n",
      " 'successful also in the context of generative methods. As such, we decided to '\n",
      " 're-evaluate the network design of StyleGAN and search for an architecture '\n",
      " 'that produces high-quality images without progressive growing. Figure 7a '\n",
      " 'shows MSG-GAN [22] , which connects the matching resolutions of the '\n",
      " 'generator and discriminator using multiple skip connections. The MSG-GAN '\n",
      " 'generator is modified to output a mipmap [41] instead of an image, and a '\n",
      " 'similar representation is computed for each real image as well. In Figure 7b '\n",
      " 'we simplify this design by upsampling and summing the contributions of RGB '\n",
      " 'outputs corresponding to different resolutions. In the discriminator, we '\n",
      " 'similarly provide the downsampled image to each resolution block of the '\n",
      " 'discriminator. We use bilinear filtering in all up and downsampling '\n",
      " 'operations. In Figure 7c we further modify the design to use residual '\n",
      " 'connections. 3 This design is similar to LAPGAN [7] without the '\n",
      " 'per-resolution discriminators employed by Denton et al. Table 2 compares '\n",
      " 'three generator and three discriminator architectures: original feedforward '\n",
      " 'networks as used in StyleGAN, skip connections, and residual networks, all '\n",
      " 'trained without progressive growing. FID and PPL are provided for each of '\n",
      " 'the 9 combinations. We can see two broad trends: skip connections in the '\n",
      " 'generator drastically improve PPL in all configurations, and a residual '\n",
      " 'discriminator network is clearly beneficial for FID. The latter is perhaps '\n",
      " 'not surprising since the structure of discriminator resembles classifiers '\n",
      " 'where residual architectures are known to be helpful. However, a residual '\n",
      " 'architecture was harmful in the generator -the lone exception was FID in '\n",
      " 'LSUN CAR when both networks were residual. For the rest of the paper we use '\n",
      " 'a skip generator and a residual discriminator, and do not use progressive '\n",
      " 'growing. This corresponds to configuration E in Table 1 , and as can be seen '\n",
      " 'table, switching to this setup significantly improves FID and PPL. The key '\n",
      " 'aspect of progressive growing, which we would like to preserve, is that the '\n",
      " 'generator will initially focus on low-resolution features and then slowly '\n",
      " 'shift its attention to finer details. The architectures in Figure 7 make it '\n",
      " 'possible for the generator to first output low resolution images that are '\n",
      " 'not affected by the higher-resolution layers in a significant way, and later '\n",
      " 'shift the focus to the higher-resolution layers as the training proceeds. '\n",
      " 'Since this is not enforced in any way, the generator will do it only if it '\n",
      " 'is beneficial. To We can see that in the beginning the network focuses on '\n",
      " 'lowresolution images and progressively shifts its focus on larger '\n",
      " 'resolutions as training progresses. In (a) the generator basically outputs a '\n",
      " '512 2 image with some minor sharpening for 1024 2 , while in (b) the larger '\n",
      " 'network focuses more on the high-resolution details. Table 3 . Improvement '\n",
      " 'in LSUN datasets measured using FID and PPL. We trained CAR for 57M images, '\n",
      " 'CAT for 88M, CHURCH for 48M, and HORSE for 100M images. analyze the behavior '\n",
      " 'in practice, we need to quantify how strongly the generator relies on '\n",
      " 'particular resolutions over the course of training. Since the skip generator '\n",
      " '(Figure 7b ) forms the image by explicitly summing RGB values from multiple '\n",
      " 'resolutions, we can estimate the relative importance of the corresponding '\n",
      " 'layers by measuring how much they contribute to the final image. In Figure '\n",
      " '8a , we plot the standard deviation of the pixel values produced by each '\n",
      " 'tRGB layer as a function of training time. We calculate the standard '\n",
      " 'deviations over 1024 random samples of w and normalize the values so that '\n",
      " 'they sum to 100%. At the start of training, we can see that the new skip '\n",
      " 'generator behaves similar to progressive growing -now achieved without '\n",
      " 'changing the network topology. It would thus be reasonable to expect the '\n",
      " 'highest resolution to dominate towards the end of the training. The plot, '\n",
      " 'however, shows that this fails to happen in practice, which indicates that '\n",
      " 'the generator may not be able to \"fully utilize\" the target resolution. To '\n",
      " 'verify this, we inspected the generated images manually and noticed that '\n",
      " 'they generally lack some of the pixel-level detail that is present in the '\n",
      " 'training datathe images could be described as being sharpened versions of '\n",
      " '512 2 images instead of true 1024 2 images. This leads us to hypothesize '\n",
      " 'that there is a capacity prob-lem in our networks, which we test by doubling '\n",
      " 'the number of feature maps in the highest-resolution layers of both '\n",
      " 'networks. 4 This brings the behavior more in line with expectations: Figure '\n",
      " '8b shows a significant increase in the contribution of the '\n",
      " 'highest-resolution layers, and Table 1 , row F shows that FID and Recall '\n",
      " 'improve markedly. Table 3 compares StyleGAN and our improved variant in '\n",
      " 'several LSUN categories, again showing clear improvements in FID and '\n",
      " 'significant advances in PPL. It is possible that further increases in the '\n",
      " 'size could provide additional benefits. Inverting the synthesis network g is '\n",
      " 'an interesting problem that has many applications. Manipulating a given '\n",
      " 'image in the latent feature space requires finding a matching latent vector '\n",
      " 'w for it first. Also, as the image quality of GANs improves, it becomes more '\n",
      " 'important to be able to attribute a potentially synthetic image to the '\n",
      " 'network that generated it. Previous research [1, 10] suggests that instead '\n",
      " 'of finding a common latent vector w, the results improve if a separate w is '\n",
      " 'chosen for each layer of the generator. The same approach was used in an '\n",
      " 'early encoder implementation [32] . While extending the latent space in this '\n",
      " 'fashion finds a closer match to a given image, it also enables projecting '\n",
      " 'arbitrary images that should have no latent representation. Focusing on the '\n",
      " 'forensic detection of generated images, we concentrate on finding latent '\n",
      " 'codes in the original, unextended latent space, as these correspond to '\n",
      " 'images that the generator could have produced. Our projection method differs '\n",
      " 'from previous methods in two ways. First, we add ramped-down noise to the '\n",
      " 'latent code during optimization in order to explore the latent space more '\n",
      " 'comprehensively. Second, we also optimize the stochastic noise inputs of the '\n",
      " 'StyleGAN generator, regularizing them to ensure they do not end up carrying '\n",
      " 'coherent signal. The regularization is based on enforcing the '\n",
      " 'autocorrelation coefficients of the noise maps to match those of unit '\n",
      " 'Gaussian noise over multiple scales. Details of our projection method can be '\n",
      " 'found in Appendix D. It is possible to train classifiers to detect '\n",
      " 'GAN-generated images with reasonably high confidence [29, 44, 40, 50] . '\n",
      " 'However, given the rapid pace of progress, this may not be a lasting '\n",
      " 'situation. Projection-based methods are unique in that they can provide '\n",
      " 'evidence, in the form of a matching latent vector, that an image was '\n",
      " 'synthesized by a specific network [2] . There is also no reason why their '\n",
      " 'effectiveness Generated Real LSUN CAR, Our config F FFHQ, Our config F '\n",
      " 'Figure 9 . LPIPS distances between original and projected images. Distance '\n",
      " 'histograms for generated images shown in blue, real images in orange. '\n",
      " 'Despite the higher image quality of our improved generator, it is much '\n",
      " 'easier to project the generated images into its latent space W. The same '\n",
      " 'projection method was used in all cases. would diminish as the quality of '\n",
      " 'synthetic images improves, unlike classifier-based methods that may have '\n",
      " 'fewer clues to work with in the future. It turns out that our improvements '\n",
      " 'to StyleGAN make it easier to detect generated images using projection-based '\n",
      " 'methods, even though the quality of generated images is higher. We measure '\n",
      " 'how well the projection succeeds by computing the LPIPS [49] distance '\n",
      " 'between original and re-synthesized image as D LPIPS [x, g(g −1 (x))], where '\n",
      " 'x is the image being analyzed andg −1 denotes the approximate projection '\n",
      " 'operation. Figure 9 shows histograms of these distances for LSUN CAR and '\n",
      " 'FFHQ datasets using the original StyleGAN and our best architecture, and '\n",
      " 'Figure 10 shows example projections. As the latter illustrates, the images '\n",
      " 'generated using our improved architecture can be projected into generator '\n",
      " 'inputs so well that they can be unambiguously attributed to the generating '\n",
      " 'network. With original StyleGAN, even though it should technically be '\n",
      " 'possible to find a matching latent vector, it appears that the latent space '\n",
      " 'is in practice too complex for this to succeed reliably. Our improved model '\n",
      " 'with smoother latent space W suffers considerably less from this problem. We '\n",
      " 'have identified and fixed several image quality issues in StyleGAN, '\n",
      " 'improving the quality further and considerably advancing the state of the '\n",
      " 'art in several datasets. In some cases the improvements are more clearly '\n",
      " 'seen in motion, as demonstrated in the accompanying video. Appendix A '\n",
      " 'includes further examples of results obtainable using our method. Despite '\n",
      " 'the improved quality, it is easier to detect images generated by our method '\n",
      " 'using projectionbased methods, compared to the original StyleGAN. Baseline '\n",
      " 'StyleGAN -generated images Our configuration F -real images Figure 10 . '\n",
      " 'Example images and their projected and re-synthesized counterparts. For each '\n",
      " 'configuration, top row shows the target images and bottom row shows the '\n",
      " 'synthesis of the corresponding projected latent vector and noise inputs. '\n",
      " 'Top: With the baseline StyleGAN, projection often finds a reasonably close '\n",
      " 'match for generated images, but especially the backgrounds differ from the '\n",
      " 'originals. Middle: The images generated using our best architecture can be '\n",
      " 'projected almost perfectly back into generator inputs, allowing unambiguous '\n",
      " 'attribution to the generating model. Bottom: Projected real images (from the '\n",
      " 'training set) show clear differences to the originals, as expected. All '\n",
      " 'tests were done using the same projection method and hyperparameters. '\n",
      " 'Training performance has also improved. At 1024 2 resolution, the original '\n",
      " 'StyleGAN (config A in Table 1 ) trains at 37 images per second on NVIDIA '\n",
      " 'DGX-1 with 8 Tesla V100 GPUs, while our config E trains 40% faster at 61 '\n",
      " 'img/s. Most of the speedup comes from simplified dataflow due to weight '\n",
      " 'demodulation, lazy regularization, and code optimizations. Config F (larger '\n",
      " 'networks) trains at 31 img/s, and is thus only slightly more expensive to '\n",
      " 'train than original StyleGAN. With config F, the total training time was 9 '\n",
      " 'days for FFHQ and 13 days for LSUN CAR. As future work, it could be fruitful '\n",
      " 'to study further improvements to the path length regularization, e.g., by '\n",
      " 'replacing the pixel-space L 2 distance with a data-driven featurespace '\n",
      " 'metric. We thank Ming-Yu Liu for an early review, Timo Viitanen for his help '\n",
      " 'with code release, and Tero Kuosmanen for compute infrastructure. We include '\n",
      " 'several large images that illustrate various aspects related to image '\n",
      " 'quality. Figure 11 shows hand-picked examples illustrating the quality and '\n",
      " 'diversity achievable using our method in FFHQ, while Figure 12 shows '\n",
      " 'uncurated results for all datasets mentioned in the paper. Figures 13 and 14 '\n",
      " 'demonstrate cases where FID and P&R give non-intuitive results, but PPL '\n",
      " 'seems to be more in line with human judgement. We also include images '\n",
      " 'relating to StyleGAN artifacts. Figure 15 shows a rare case where the blob '\n",
      " 'artifact fails to appear in StyleGAN activations, leading to a seriously '\n",
      " 'broken image. Figure 16 visualizes the activations inside Table 1 '\n",
      " 'configurations A and F. It is evident that progressive growing leads to '\n",
      " 'higher-frequency content in the intermediate layers, compromising shift '\n",
      " 'invariance of the network. We hypothesize that this causes the observed '\n",
      " 'uneven location preference for details when progressive growing is used. We '\n",
      " 'implemented our techniques on top of the official TensorFlow implementation '\n",
      " 'of StyleGAN 5 corresponding to configuration A in Table 1 . We kept most of '\n",
      " 'the details unchanged, including the dimensionality of Z and W (512), '\n",
      " 'mapping network architecture (8 fully connected layers, 100× lower learning '\n",
      " 'rate), equalized learning rate for all trainable parameters [23] , leaky '\n",
      " 'ReLU activation with α = 0.2, bilinear filtering [48] in all up/downsampling '\n",
      " 'layers [24] , minibatch standard deviation layer at the end of the '\n",
      " 'discriminator [23] , exponential moving average of generator weights [23] , '\n",
      " 'style mixing regularization [24] , nonsaturating logistic loss [15] with R 1 '\n",
      " 'regularization [30] , Adam optimizer [25] with the same hyperparameters (β 1 '\n",
      " '= 0, β 2 = 0.99, = 10 −8 , minibatch = 32), and training datasets [24, 43] . '\n",
      " 'We performed all training runs on NVIDIA DGX-1 with 8 Tesla V100 GPUs using '\n",
      " 'Tensor-Flow 1.14.0 and cuDNN 7.4.2. Generator redesign In configurations B-F '\n",
      " 'we replace the original StyleGAN generator with our revised architecture. In '\n",
      " 'addition to the changes highlighted in Section 2, we initialize components '\n",
      " 'of the constant input c 1 using N (0, 1) and simplify the noise broadcast '\n",
      " 'operations to use a single shared scaling factor for all feature maps. We '\n",
      " 'employ weight modulation and demodulation in all convolution layers, except '\n",
      " 'for the output layers (tRGB in Figure 7) where we leave out the '\n",
      " 'demodulation. With 1024 2 output resolution, the generator contains a total '\n",
      " 'of 18 affine transformation layers where the first one corresponds to 4 2 '\n",
      " 'resolution, the next two correspond to 8 2 , and so forth. Tables 1 and 3 . '\n",
      " 'The images correspond to random outputs produced by our generator (config '\n",
      " 'F), with truncation applied at all resolutions using ψ = 0.5 [24] . Figure '\n",
      " '13 . Uncurated examples from two generative models trained on LSUN CAT '\n",
      " 'without truncation. FID, precision, and recall are similar for models 1 and '\n",
      " '2, even though the latter produces cat-shaped objects more often. Perceptual '\n",
      " 'path length (PPL) indicates a clear preference for model 2. Model 1 '\n",
      " 'corresponds to configuration A in Table 3 , and model 2 is an early training '\n",
      " 'snapshot of configuration F. Figure 14 . Uncurated examples from two '\n",
      " 'generative models trained on LSUN CAR without truncation. FID, precision, '\n",
      " 'and recall are similar for models 1 and 2, even though the latter produces '\n",
      " 'car-shaped objects more often. Perceptual path length (PPL) indicates a '\n",
      " 'clear preference for model 2. Model 1 corresponds to configuration A in '\n",
      " 'Table 3 , and model 2 is an early training snapshot of configuration F. '\n",
      " 'Feature map 128 2 Feature map 256 2 Feature map 512 2 Generated image Figure '\n",
      " '15 . An example of the importance of the droplet artifact in StyleGAN '\n",
      " 'generator. We compare two generated images, one successful and one severely '\n",
      " 'corrupted. The corresponding feature maps were normalized to the viewable '\n",
      " 'dynamic range using instance normalization. For the top image, the droplet '\n",
      " 'artifact starts forming in 64 2 resolution, is clearly visible in 128 2 , '\n",
      " 'and increasingly dominates the feature maps in higher resolutions. For the '\n",
      " 'bottom image, 64 2 is qualitatively similar to the top row, but the droplet '\n",
      " 'does not materialize in 128 2 . Consequently, the facial features are '\n",
      " 'stronger in the normalized feature map. This leads to an overshoot in 256 2 '\n",
      " ', followed by multiple spurious droplets forming in subsequent resolutions. '\n",
      " 'Based on our experience, it is rare that the droplet is missing from '\n",
      " 'StyleGAN images, and indeed the generator fully relies on its existence. '\n",
      " 'Weight demodulation Considering the practical implementation of Equations 1 '\n",
      " 'and 3, it is important to note that the resulting set of weights will be '\n",
      " 'different for each sample in a minibatch, which rules out direct '\n",
      " 'implementation using standard convolution primitives. Instead, we choose to '\n",
      " 'employ grouped convolutions [26] that were originally proposed as a way to '\n",
      " 'reduce computational costs by dividing the input feature maps into multiple '\n",
      " 'independent groups, each with their own dedicated set of weights. We '\n",
      " 'implement Equations 1 and 3 by temporarily reshaping the weights and '\n",
      " 'activations so that each convolution sees one sample with N groups -instead '\n",
      " 'of N samples with one group. This approach is highly efficient because the '\n",
      " 'reshaping operations do not actually modify the contents of the weight and '\n",
      " 'activation tensors. Lazy regularization In configurations C-F we employ lazy '\n",
      " 'regularization (Section 3.1) by evaluating the regularization terms (R 1 and '\n",
      " 'path length) in a separate regularization pass that we execute once every k '\n",
      " 'training iterations. We share the internal state of the Adam optimizer '\n",
      " 'between the main loss and the regularization terms, so that the opti-mizer '\n",
      " 'first sees gradients from the main loss for k iterations, followed by '\n",
      " 'gradients from the regularization terms for one iteration. To compensate for '\n",
      " 'the fact that we now perform k+1 training iterations instead of k, we adjust '\n",
      " 'the optimizer hyperparameters λ = c · λ, β 1 = (β 1 ) c , and β 2 = (β 2 ) c '\n",
      " ', where c = k/(k + 1). We also multiply the regularization term by k to '\n",
      " 'balance the overall magnitude of its gradients. We use k = 16 for the '\n",
      " 'discriminator and k = 8 for the generator. Path length regularization '\n",
      " 'Configurations D-F include our new path length regularizer (Section 3.2). We '\n",
      " 'initialize the target scale a to zero and track it on a per-GPU basis as the '\n",
      " 'exponential moving average of J T w y 2 using decay coefficient β pl = 0.99. '\n",
      " 'We weight our regularization term by where r specifies the output resolution '\n",
      " '(e.g. r = 1024). We have found these parameter choices to work reliably '\n",
      " 'across all configurations and datasets. To ensure that our regularizer '\n",
      " 'interacts correctly with style mixing regularization, we compute it as an '\n",
      " 'average of all individual layers of the synthesis network. Appendix C '\n",
      " 'provides detailed analysis of the effects of our regularizer on the mapping '\n",
      " 'between W and image space. Progressive growing In configurations A-D we use '\n",
      " 'progressive growing with the same parameters as Karras et al. [24] (start at '\n",
      " '8 2 resolution and learning rate λ = 10 −3 , train for 600k images per '\n",
      " 'resolution, fade in next resolution for 600k images, increase learning rate '\n",
      " 'gradually by 3×). In configurations E-F we disable progressive growing and '\n",
      " 'set the learning rate to a fixed value λ = 2 · 10 −3 , which we found to '\n",
      " 'provide the best results. In addition, we use output skips in the generator '\n",
      " 'and residual connections in the discriminator as detailed in Section 4.1. '\n",
      " 'Dataset-specific tuning Similar to Karras et al. [24] , we augment the FFHQ '\n",
      " 'dataset with horizontal flips to effectively increase the number of training '\n",
      " 'images from 70k to 140k, and we do not perform any augmentation for the LSUN '\n",
      " 'datasets. We have found that the optimal choices for the training length and '\n",
      " 'R 1 regularization weight γ tend to vary considerably between different '\n",
      " 'datasets and configurations. We use γ = 10 for all training runs except for '\n",
      " 'configuration E in Table 1 , as well as LSUN CHURCH and LSUN HORSE in Table '\n",
      " '3 , where we use γ = 100. It is possible that further tuning of γ could '\n",
      " 'provide additional benefits. Performance optimizations We profiled our '\n",
      " 'training runs extensively and found that -in our case -the default '\n",
      " 'primitives for image filtering, up/downsampling, bias addition, and leaky '\n",
      " 'ReLU had surprisingly high overheads in terms of training time and GPU '\n",
      " 'memory footprint. This motivated us to optimize these operations using '\n",
      " 'hand-written CUDA kernels. We implemented filtered up/downsampling as a '\n",
      " 'single fused operation, and bias and activation as another one. In '\n",
      " 'configuration E at 1024 2 resolution, our optimizations improved the overall '\n",
      " 'training time by about 30% and memory footprint by about 20%. The path '\n",
      " 'length regularizer described in Section 3.2 is of the form: where y ∈ R M is '\n",
      " 'a unit normal distributed random variable in the space of generated images '\n",
      " '(of dimension M = 3wh, namely the RGB image dimensions), J w ∈ R M ×L is the '\n",
      " 'Jacobian matrix of the generator function g : R L → R M at a latent space '\n",
      " 'point w ∈ R L , and a ∈ R is a global value that expresses the desired scale '\n",
      " 'of the gradients. The value of this prior is minimized when the inner '\n",
      " 'expectation over y is minimized at every latent space point w separately. In '\n",
      " 'this subsection, we show that the inner expectation is (approximately) '\n",
      " 'minimized when the Jacobian matrix J w is orthogonal, up to a global scaling '\n",
      " 'factor. The general strategy is to use the well-known fact that, in high '\n",
      " 'dimensions L, the density of a unit normal distribution is concentrated on a '\n",
      " 'spherical shell of radius √ L. The inner expectation is then minimized when '\n",
      " 'the matrix J T w scales the function under expectation to have its minima at '\n",
      " 'this radius. This is achieved by any orthogonal matrix (with suitable global '\n",
      " 'scale that is the same at every w). We begin by considering the inner '\n",
      " 'expectation We first note that the radial symmetry of the distribution of y, '\n",
      " 'as well as of the l 2 norm, allows us to focus on diagonal matrices only. '\n",
      " 'This is seen using the Singular Value Decomposition J T w = UΣV T , where U '\n",
      " '∈ R L×L and V ∈ R M ×M are orthogonal matrices, andΣ = [Σ 0] is a horizontal '\n",
      " 'concatenation of a diagonal matrix Σ ∈ R L×L and a zero matrix 0 ∈ R L×(M '\n",
      " '−L) [14] . Because rotating a unit normal random variable by an orthogonal '\n",
      " 'matrix leaves the distribution unchanged, and rotating a vector leaves its '\n",
      " 'norm unchanged, the expression simplifies to Furthermore, the zero matrix '\n",
      " 'inΣ drops the dimensions of y beyond L, effectively marginalizing its '\n",
      " 'distribution over those dimensions. The marginalized distribution is again a '\n",
      " 'unit normal distribution over the remaining L dimensions. We are then left '\n",
      " 'to consider the minimization of the expression over diagonal square matrices '\n",
      " 'Σ ∈ R L×L , whereỹ is unit normal distributed in dimension L. To summarize, '\n",
      " 'all matrices J T w that share the same singular values with Σ produce the '\n",
      " 'same value for the original loss. Next, we show that this expression is '\n",
      " 'minimized when the diagonal matrix Σ has a specific identical value at every '\n",
      " 'diagonal entry, i.e., it is a constant multiple of an identity matrix. We '\n",
      " 'first write the expectation as an integral over the probability density ofỹ: '\n",
      " 'Observing the radially symmetric form of the density, we change into a polar '\n",
      " 'coordinatesỹ = rφ, where r ∈ R + is the distance from origin, and φ ∈ S L−1 '\n",
      " 'is a unit vector, i.e., a point on the L − 1-dimensional unit sphere. This '\n",
      " 'change of variables introduces a Jacobian factor r L−1 : The probability '\n",
      " 'density (2π) −L/2 r L−1 exp − r 2 2 is then an L-dimensional unit normal '\n",
      " 'density expressed in polar coordinates, dependent only on the radius and not '\n",
      " 'on the angle. A standard argument by Taylor approximation shows that when L '\n",
      " 'is high, for any φ the density is well approximated by density (2πe/L) −L/2 '\n",
      " 'exp − 1 2 (r − µ) 2 /σ 2 , which is a (unnormalized) one-dimensional normal '\n",
      " 'density in r, centered at µ = √ L of standard deviation σ = 1/ √ 2 [4] . In '\n",
      " 'other words, the density of the L-dimensional unit normal distribution is '\n",
      " 'concentrated on a shell of radius √ L. Substituting this density into the '\n",
      " 'integral, the loss becomes approximately where the approximation becomes '\n",
      " 'exact in the limit of infinite dimension L. To minimize this loss, we set Σ '\n",
      " 'such that the function (r Σφ 2 − a) 2 obtains minimal values on the '\n",
      " 'spherical shell of radius √ L. This is achieved by Σ = a √ L I, whereby the '\n",
      " 'function becomes constant in φ and the expression reduces to where A(S) is '\n",
      " 'the surface area of the unit sphere (and like the other constant factors, '\n",
      " 'irrelevant for minimization). Note that the zero of the parabola (r − √ L) 2 '\n",
      " 'coincides with the maximum of the probability density, and therefore this '\n",
      " 'choice of Σ minimizes the inner integral in Eq. 7 separately for every φ. In '\n",
      " 'summary, we have shown that -assuming a high dimensionality L of the latent '\n",
      " 'space -the value of the path length prior (Eq. 6) is minimized when all '\n",
      " 'singular values of the Jacobian matrix of the generator are equal to a '\n",
      " 'global constant, at every latent space point w, i.e., they are orthogonal up '\n",
      " 'to a globally constant scale. While in theory a merely scales the values of '\n",
      " 'the mapping without changing its properties and could be set to a fixed '\n",
      " 'value (e.g., 1), in practice it does affect the dynamics of the training. If '\n",
      " 'the imposed scale does not match the scale induced by the random '\n",
      " 'initialization of the network, the training spends its critical early steps '\n",
      " 'in pushing the weights towards the required overall magnitudes, rather than '\n",
      " 'enforcing the actual objective of interest. This may degrade the internal '\n",
      " 'state of the network weights and lead to sub-optimal performance in later '\n",
      " 'training. Empirically we find that setting a fixed scale reduces the '\n",
      " 'consistency of the training results across training runs and datasets. '\n",
      " 'Instead, we set a dynamically based on a running average of the existing '\n",
      " 'scale of the Jacobians, namely a ≈ E w,y J T w y 2 . With this choice the '\n",
      " 'prior targets the scale of the local Jacobians towards whatever global '\n",
      " 'average already exists, rather than forcing a specific global average. This '\n",
      " 'also eliminates the need to measure the appropriate scale of the Jacobians '\n",
      " 'Figure 17 . The mean and standard deviation of the magnitudes of sorted '\n",
      " 'singular values of the Jacobian matrix evaluated at random latent space '\n",
      " 'points w, with largest eigenvalue normalized to 1. In both datasets, path '\n",
      " 'length regularization (Config D) and novel architecture (Config F) exhibit '\n",
      " 'better conditioning; notably, the effect is more pronounced in the Cars '\n",
      " 'dataset that contains much more variability, and where path length '\n",
      " 'regularization has a relatively stronger effect on the PPL metric (Table 1) '\n",
      " '. explicitly, as is done by Odena et al. [33] who consider a related '\n",
      " 'conditioning prior. Figure 17 shows empirically measured magnitudes of '\n",
      " 'singular values of the Jacobian matrix for networks trained with and without '\n",
      " 'path length regularization. While orthogonality is not reached, the '\n",
      " 'eigenvalues of the regularized network are closer to one another, implying '\n",
      " 'better conditioning, with the strength of the effect correlated with the PPL '\n",
      " 'metric (Table 1 ). In the previous subsection, we found that the prior '\n",
      " 'encourages the Jacobians of the generator mapping to be everywhere '\n",
      " 'orthogonal. While Figure 17 shows that the mapping does not satisfy this '\n",
      " 'constraint exactly in practice, it is instructive to consider what global '\n",
      " 'properties the constraint implies for mappings that do. Without loss of '\n",
      " 'generality, we assume unit global scale for the matrices to simplify the '\n",
      " 'presentation. The key property is that that a mapping g : R L → R M with '\n",
      " 'everywhere orthogonal Jacobians preserves the lengths of curves. To see '\n",
      " 'this, let u : [t 0 , t 1 ] → R L parametrize a curve in the latent space. '\n",
      " 'Mapping the curve through the generator g, we obtain a curveũ = g • u in the '\n",
      " 'space of images. Its arc length is where prime denotes derivative with '\n",
      " 'respect to t. By chain rule, this equals where J g ∈ R L×M is the Jacobian '\n",
      " 'matrix of g evaluated at u(t). By our assumption, the Jacobian is '\n",
      " 'orthogonal, and consequently it leaves the 2-norm of the vector u (t) '\n",
      " 'unaffected: This is the length of the curve u in the latent space, prior to '\n",
      " 'mapping with g. Hence, the lengths of u andũ are equal, and so g preserves '\n",
      " 'the length of any curve. In the language of differential geometry, g '\n",
      " 'isometrically embeds the Euclidean latent space R L into a submanifold M in '\n",
      " 'R M -e.g., the manifold of images representing faces, embedded within the '\n",
      " 'space of all possible RGB images. A consequence of isometry is that straight '\n",
      " 'line segments in the latent space are mapped to geodesics, or shortest '\n",
      " 'paths, on the image manifold: a straight line v that connects two latent '\n",
      " 'space points cannot be made any shorter, so neither can there be a shorter '\n",
      " 'on-manifold image-space path between the corresponding images than g • v. '\n",
      " 'For example, a geodesic on the manifold of face images is a continuous morph '\n",
      " 'between two faces that incurs the minimum total amount of change (as '\n",
      " 'measured by l 2 difference in RGB space) when one sums up the image '\n",
      " 'difference in each step of the morph. Isometry is not achieved in practice, '\n",
      " 'as demonstrated in empirical experiments in the previous subsection. The '\n",
      " 'full loss function of the training is a combination of potentially '\n",
      " 'conflicting criteria, and it is not clear if a genuinely isometric mapping '\n",
      " 'would be capable of expressing the image manifold of interest. Nevertheless, '\n",
      " 'a pressure to make the mapping as isometric as possible has desirable '\n",
      " 'consequences. In particular, it discourages unnecessary \"detours\": in a '\n",
      " 'nonconstrained generator mapping, a latent space interpolation between two '\n",
      " 'similar images may pass through any number of distant images in RGB space. '\n",
      " 'With regularization, the mapping is encouraged to place distant images in '\n",
      " 'different regions of the latent space, so as to obtain short image paths '\n",
      " 'between any two endpoints. Given a target image x, we seek to find the '\n",
      " 'corresponding w ∈ W and per-layer noise maps denoted n i ∈ R ri×ri where i '\n",
      " 'is the layer index and r i denotes the resolution of the ith noise map. The '\n",
      " 'baseline StyleGAN generator in 1024×1024 resolution has 18 noise inputs, '\n",
      " 'i.e., two for each resolution from 4×4 to 1024×1024 pixels. Our improved '\n",
      " 'architecture has one fewer noise input because we do not add noise to the '\n",
      " 'learned 4×4 constant (Figure 2) . Before optimization, we compute µ w = E z '\n",
      " 'f (z) by running 10 000 random latent codes z through the mapping network f '\n",
      " '. We also approximate the scale of W by computing σ 2 w = E z f (z) − µ w 2 '\n",
      " '2 , i.e., the average square Euclidean distance to the center. At the '\n",
      " 'beginning of optimization, we initialize w = µ w and n i = N (0, I) for all '\n",
      " 'i. The trainable parameters are the components of w as well as all '\n",
      " 'components in all noise maps n i . The optimization is run for 1000 '\n",
      " 'iterations using Adam optimizer [25] with default parameters. Maximum '\n",
      " 'learning rate is λ max = 0.1, and it is ramped up from zero linearly during '\n",
      " 'the first 50 iterations and ramped down to zero using a cosine schedule '\n",
      " 'during the last 250 iterations. In the first three quarters of the '\n",
      " 'optimization we add Gaussian noise to w when evaluating the loss function as '\n",
      " 'w = w + N (0, 0.05 σ w t 2 ), where t goes from one to zero during the first '\n",
      " '750 iterations. This adds stochasticity to the optimization and stabilizes '\n",
      " 'the finding of the global optimum. Given that we are explicitly optimizing '\n",
      " 'the noise maps, we must be careful to avoid the optimization from sneaking '\n",
      " 'actual signal into them. Thus we include several noise map regularization '\n",
      " 'terms in our loss function, in addition to an image quality term. The image '\n",
      " 'quality term is the LPIPS [49] distance between target image x and the '\n",
      " 'synthesized image: L image = D LPIPS [x, g(w, n 0 , n 1 , . . .)]. For '\n",
      " 'increased performance and stability, we downsample both images to 256×256 '\n",
      " 'resolution before computing the LPIPS distance. Regularization of the noise '\n",
      " 'maps is performed on multiple resolution scales. For this purpose, we form '\n",
      " 'for each noise map greater than 8×8 in size a pyramid down to 8×8 resolution '\n",
      " 'by averaging 2×2 pixel neighborhoods and multiplying by 2 at each step to '\n",
      " 'retain the expected unit variance. These downsampled noise maps are used for '\n",
      " 'regularization only and have no part in synthesis. Let us denote the '\n",
      " 'original noise maps by n i,0 = n i and the downsampled versions by n i,j>0 . '\n",
      " 'Similarly, let r i,j be the resolution of an original (j = 0) or downsampled '\n",
      " '(j > 0) noise map so that r i,j+1 = r i,j /2. The regularization term for '\n",
      " 'noise map n i,j is then x,y n i,j (x, y) · n i,j (x − 1, y) where the noise '\n",
      " 'map is considered to wrap at the edges. The regularization term is thus sum '\n",
      " 'of squares of the resolutionnormalized autocorrelation coefficients at one '\n",
      " 'pixel shifts horizontally and vertically, which should be zero for a '\n",
      " 'normally distributed signal. The overall loss term is then L total = L image '\n",
      " '+ α i,j L i,j . In all our tests, we have used noise regularization weight α '\n",
      " '= 10 5 . In addition, we renormalize all noise maps to zero mean and unit '\n",
      " 'variance after each optimization step. Figure 18 illustrates the effect of '\n",
      " 'noise regularization on the resulting noise maps. Figure 18 . Effect of '\n",
      " 'noise regularization in latent-space projection where we also optimize the '\n",
      " 'contents of the noise inputs of the synthesis network. Top to bottom: target '\n",
      " 'image, re-synthesized image, contents of two noise maps at different '\n",
      " 'resolutions. When regularization is turned off in this test, we only '\n",
      " 'normalize the noise maps to zero mean and unit variance, which leads the '\n",
      " 'optimization to sneak signal into the noise maps. Enabling the noise '\n",
      " 'regularization prevents this. The model used here corresponds to '\n",
      " 'configuration F in Table 1 .')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "pprint(dmdd[\"content\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
